# 摘要
大型语言模型（LLMs）面临一个固有难题：其知识被局限在训练数据范围内，这一限制加上高昂的重训练成本，使其无法提供实时更新的响应。为解决这些问题，检索增强生成（RAG）技术通过外部知识库对 LLMs 基于静态训练的知识进行了补充，RAG 包含三个阶段：（i）索引阶段，构建数据库以实现文本嵌入的相似度检索；（ii）检索阶段，根据用户查询从数据库中搜索并获取相关数据；（iii）生成阶段，结合用户查询与检索到的数据生成响应。
其中 RAG 的检索阶段是推理流水线中的关键性能瓶颈，该阶段包含两个步骤：（i）将用户查询映射为嵌入向量；（ii）通过近似最近邻搜索（ANNS）算法在数据库中查找语义最相似的嵌入向量以定位相关内容，由于数据库规模庞大，ANNS 会在主机与存储系统之间产生巨大的数据移动开销。
为缓解这一问题，已有研究提出了存储内处理（ISP）技术，通过在存储系统内部执行计算来加速 ANNS 负载，但现有基于 ISP 的 ANNS 方案存在以下局限：（i）所采用的算法并非针对 ISP 系统定制；（ii）未对 ANNS 筛选之后的其它操作进行加速；（iii）对存储系统进行了大量硬件改造，既限制了性能也阻碍了实际落地。
我们提出 REIS—— 首个专为 RAG 设计的存储内检索系统，通过三项核心机制解决了现有方案的不足：首先，REIS 设计了一种数据库布局，将嵌入向量与对应的文档关联起来以实现高效检索；其次，引入了 ISP 定制化的算法与数据布局技术以实现高效 ANNS：（i）将嵌入向量分布在存储系统的所有平面上以充分利用并行性；（ii）采用轻量级闪存转换层（FTL）提升性能；第三，利用存储系统内部已有的计算资源构建 ANNS 引擎，无需额外硬件改造。这三项机制构成了一个协同框架，大幅提升了 RAG 流水线的性能与能效，与高端服务器级系统相比，REIS 使检索阶段的性能平均提升 13 倍、能效平均提升 55 倍，同时它在无需硬件改造的前提下实现了优于现有 ISP-based ANNS 加速器的性能，更易于在 RAG 流水线中落地应用。
# 背景知识
## RAG
![image.png](https://raw.githubusercontent.com/waibibab-cs/blog_img/main/cdnimg/20260101121407.png)
过去十年间，大型语言模型（LLMs）的快速发展推动了其广泛应用，但其存在的局限是：它们只能基于训练集中的数据生成响应。而训练所需的高昂成本与硬件要求使得基于新数据的频繁重训练不切实际，进而限制了 LLMs 在特定领域与实时场景 中的有效性。

检索增强生成（RAG）为这一问题提供了极具吸引力的解决方案：它利用信息检索技术，将文档数据库中的相关内容输入 LLM 以辅助文本生成。在推理阶段，RAG 系统会从数据库中检索与用户查询相关的文档，以此补充 LLMs 的训练知识，生成符合语境的响应。近期多项研究表明，RAG 可应用于医疗 、法律 、金融、科研等领域。

RAG 的通用工作流由三个阶段构成：（i）索引、（ii）检索、（iii）生成。首先，索引是一个离线过程，用于构建高维嵌入的向量数据库。索引阶段一般采用聚类相似数据或构建类图结构的算法，以便后续对数据执行搜索操作。其次，针对每个输入查询，检索阶段会识别与查询语义相关的文档块。为此，RAG 采用 “稠密检索”机制：将查询编码到与文档块相同的向量空间中，再对查询与数据库嵌入执行相似度搜索。第三，生成阶段将识别出的文档块与查询一同输入 LLM，生成最终响应。
## ANNS
对于 RAG 流水线而言，兼具**高召回率**与**低延迟**的检索器至关重要 —— 这是因为它（i）决定了生成响应的质量，（ii）处于响应生成流程的关键路径中。而检索阶段是RAG 流水线的核心性能瓶颈，为加速检索阶段，因此RAG 通常结合近似最近邻搜索（ANNS）技术以牺牲少量检索精度为代价，实现更快的相似度搜索。

实现 ANNS 的两类主流方法为：（i）量化法；（ii）算法优化法

量化法通过压缩数据减少存储开销、提升计算速度。例如，乘积量化（PQ）将高维嵌入向量切分为多个子向量，为每个子向量分配对应聚类，再将各聚类编号拼接为新向量，以此表征原始向量；二进制量化（BQ）则将每个嵌入分量从原始浮点精度（如 32 位浮点）压缩至 1 比特，实现 32 倍的压缩比。近期研究表明，二进制量化可将 ANNS 速度提升至多 40 倍，若搭配低成本的重排序步骤，对召回率的影响微乎其微。

算法优化法通过聚类或构建类图数据结构对数据进行组织，无需遍历全量数据库即可高效完成搜索。例如，倒排文件（IVF）算法 将嵌入向量按聚类分组，每个聚类由一个质心向量表征；针对目标查询向量执行检索时，先通过粗粒度搜索筛选出与查询向量最相似的聚类质心，再对这些聚类内的所有嵌入向量执行细粒度搜索，即可近似得到与查询向量最邻近的结果。此外，主流算法还包括：（i）分层可导航小世界网络（HNSW），该算法构建多层图结构，上层图实现粗粒度检索导向、下层图完成细粒度检索；（ii）局部敏感哈希（LSH），该算法能以高概率将相似嵌入向量映射至同一哈希桶中。
## NAND Flash SSD经典结构
见文章：[https://waibibab-cs.github.io/post/NAND%20Flash%20SSD%20-jing-dian-jie-gou.html](https://waibibab-cs.github.io/post/NAND%20Flash%20SSD%20-jing-dian-jie-gou.html)
## 存内处理（ISP）介绍
ANNS 计算复杂度的降低，使得 I/O 数据传输成为限制搜索性能的主要瓶颈 ，因此多项研究提出 “存储内处理（ISP）” 作为加速 ANNS 类负载的可行方案，尽管现有部分软件与硬件方案可降低存储开销，但这些方法要么无法扩展（如量化方法），要么难以扩展（如内存扩容 ），由此我们认为存储内处理（ISP）技术是从根本上解决 RAG 流水线中关键 I/O 数据移动瓶颈的核心手段。

ISP 技术主要通过两种方式实现运算：一是利用固态硬盘（SSD）中已搭载的嵌入式通用计算核心执行计算；二是在闪存芯片旁部署硬件加速器 完成运算。对于前者，已有研究提出各类方案，借助 SSD 的嵌入式核心实现过滤、聚合、加密等计算操作。但这类通用嵌入式核心仅适用于简单运算，因其核心职责是执行闪存转换层（FTL）逻辑、处理 I/O 请求。对于后者，这类硬件加速器虽能带来大幅性能提升，却会增加 SSD 的芯片面积与功耗开销。

现有基于 ISP 的 ANNS 加速器（如：ICE、VStore、NDSEARCH、Proxima等）存在三项关键局限，阻碍了其在 RAG 负载中的应用：1. 现有方案采用的搜索算法会导致 ISP 系统性能下降，例如ISP加速器 VStore、NDSEARCH所使用的图基算法通过图遍历实现搜索，而图遍历是一个串行过程 —— 算法会基于当前访问的顶点分析确定下一个待访问的顶点，这种过程会产生不规则的数据访问模式，增加了 ISP 系统中优化与高效执行的难度；2. 现有 ISP 方案仅聚焦于加速 RAG 应用中的 ANNS 搜索阶段，未对文档检索阶段等后续阶段进行优化，而 3.2 节将说明，文档检索阶段为 RAG 流水线带来了显著延迟；3. 现有 ISP 方案在加速 ANNS 应用时，引入了巨大的存储或硬件开销。
# 工作动机
## RAG流水线的瓶颈
![image.png](https://raw.githubusercontent.com/waibibab-cs/blog_img/main/cdnimg/20260101125742.png)
为挖掘潜在性能开销组成，我们量化了各阶段对 RAG 流水线端到端延迟的占比贡献。图 2 展示了 RAG 流水线中各类操作对端到端执行时间的占比情况，核心结论有二：其一，数据集加载耗时占流水线总延迟的比重极大；其二，数据集加载延迟的占比随数据集规模扩大而提升。注意，这个“数据集加载”指的是RAG 检索前，把 SSD 里的全量嵌入向量数据库（包含关联的文档），通过 PCIe 总线全量读到主机内存 / 计算单元的 I/O 操作。由此我们得出结论：检索阶段的数据集加载为 RAG 流水线带来了显著延迟，成为性能瓶颈，在大规模数据集场景下尤为突出。我们将该瓶颈定义为RAG 流水线的 I/O 数据移动瓶颈。
## 现有RAG优化方案的限制
本小节探讨现有各类优化方案在缓解 RAG 流水线 I/O 数据移动瓶颈时存在的局限性。

**批处理**：一种可行方案是在执行检索前对多个查询进行批处理，以分摊数据集加载开销。但该技术的实际效果受限，原因在于为提升生成质量，不同领域的查询（如医疗、法律、金融）需基于专属的领域数据集执行检索。

**量化技术**：乘积量化（PQ）、二进制量化（BQ）等量化手段可降低 RAG 应用的存储开销。近期研究表明，二进制量化能在存储开销与召回率之间实现良好折中。然而尽管量化可大幅缩减嵌入向量的体积，却无法对文档块进行压缩，例如，wiki_en 数据集经嵌入向量二进制量化后，传输的 14GB 数据中，文档块占比达 9GB。因此我们得出结论：量化技术虽有助于缓解 I/O 数据移动瓶颈，却无法彻底消除该问题。

**算法优化**：近似最近邻搜索（ANNS）算法通常借助复杂索引提升检索性能、缩短搜索耗时，但存储这类索引的数据结构，往往比暴力检索所用的扁平索引体积更大，反而可能加剧 I/O 数据移动瓶颈。混合式 ANNS 算法试图解决这一瓶颈，其做法是将索引存储于固态硬盘中，按需加载部分索引至内存执行距离计算。其中 SPANN是混合式 ANNS 方案中性能与精度折中效果最优的技术，可通过小容量内存（如 32GB）实现对固态硬盘中 TB 级数据集的检索加速。具体而言，SPANN 将嵌入向量聚类后存储于固态硬盘，仅将聚类质心留存于内存。我们针对 SPANN 开展了实验研究，发现这类方案存在两大核心局限：其一，要实现理想的召回率与精度折中，需选取大量聚类质心，这会增加内存开销并降低性能。例如，在 HotpotQA 数据集上实现 0.92 的 10 条召回率（Recall@10），需将全部嵌入向量中 24% 的向量作为质心驻留内存，相较暴力检索仅实现 22% 的性能提升，这一结论也与该算法的原始研究相符；其二，SPANN 这类混合式 ANNS 算法仅针对嵌入向量优化存储与检索流程，并未对向量数据库的文档块检索做相应优化。由此我们认为，混合式 ANNS 算法也无法从根本上缓解 I/O 数据移动瓶颈。

**内存扩容**：存储与主机间的数据移动为 RAG 检索阶段带来了显著延迟。Compute Express Link（CXL）等内存扩容技术可实现超大容量内存配置，理论上可将 RAG 数据集全程驻留于内存中。但这类方案存在两大关键缺陷：第一，内存每 GB 单位成本远高于闪存存储；第二，这类方案不具备可持续性 —— 数据集规模的持续增长，加之领域专属应用的数据集数量不断增加，终将耗尽此类系统的内存容量。

**存储内 ANNS 加速**：已有研究提出存储内处理（ISP）技术，旨在缓解 ANNS 核心运算中的 I/O 数据移动瓶颈。尽管 ANNS 是 RAG 的核心组件，但现有基于 ISP 的 ANNS 加速器仍无法彻底消除 RAG 的 I/O 数据移动瓶颈，核心原因有三：其一，现有 ANNS 加速方案采用分层可导航小世界网络（HNSW）、DiskANN等图基算法，通过图遍历查找相似近邻。图遍历过程中，算法需基于当前顶点的分析结果确定下一个待访问顶点，由此产生不规则的数据访问模式，因通道与闪存芯片冲突带来高昂开销，导致固态硬盘的内部带宽无法充分利用；其二，现有基于 ISP 的 ANNS 加速器仅聚焦于加速检索运算，未能为关联文档的读取提供高效支持，而如图 2所示，数据集加载步骤为 RAG 检索带来了显著延迟；其三，ICE、DeepStore等研究方案引入了巨大的存储与硬件开销。例如，ICE为在闪存芯片内执行计算，采用了无需纠错即可容忍数据错误的存储格式，该格式使 8 比特、4 比特精度的数据分别产生 32 倍、8 倍的存储开销，导致存储成本居高不下；又如 DeepStore，其在存储系统中引入基于脉动阵列的架构，通过运行深度神经网络执行查询匹配，由此产生了显著的芯片面积与功耗开销。综上，这些局限性阻碍了基于 ISP 的加速技术在 RAG 流水线中的落地应用。
# REIS设计
![image.png](https://raw.githubusercontent.com/waibibab-cs/blog_img/main/cdnimg/20260101161038.png)
本节阐述 REIS 的核心设计思路，旨在解决上述各类问题。图 4 展示了 REIS 的系统整体架构及其核心机制，具体包括三方面：第一，REIS 设计了一种向量数据库布局，实现嵌入向量与文档的关联，支持高效的文档检索；第二，REIS 在存储内处理（ISP）系统中适配实现倒排文件（IVF）算法，提升检索阶段的端到端性能；第三，通过存储内 ANNS 引擎，高效执行 ANNS 核心检索运算。
## 向量数据库布局设计
REIS 设计了一种向量数据库布局，对嵌入向量与文档进行分布式存储并建立关联，最大化存储内计算的数访问并行性。该数据库布局实现三大核心功能：（i）将向量数据库拆分至索引区与文档区；（ii）为每个嵌入向量与其关联文档块建立低开销的关联映射；（iii）支持对各数据集的粗粒度访问，避免闪存转换层（FTL）的频繁调用。
### 数据库分布式存储
在 RAG 检索阶段，ANNS 核心模块需对数据库嵌入向量执行距离计算，因此嵌入向量的访问频次远高于文档。基于这一特征，REIS 通过三种方式实现数据库分布式存储，提升嵌入向量的访问效率：第一，将嵌入向量与文档分别映射至 NAND 闪存阵列的两个独立区域，即嵌入向量区（图 4 中①）与文档区（图 4 中②）；第二，采用并行优先页分配策略，将嵌入向量均匀分布至存储系统的所有平面，充分利用并行能力；第三，根据不同的文档分块粒度 ，为每个文档块分配独立的 4KB 子页或 16KB 物理页。
### 混合SSD架构设计
现代 SSD 普遍采用三层单元闪存（TLC），依托纠错码（ECC）实现高密度存储与数据完整性保障，但其纠错操作需将数据传输至 SSD 控制器的嵌入式核心执行。由于REIS 的运算均在存储系统的plane与die内完成，若在控制器端执行 ECC 纠错，将产生巨大的数据移动开销，为解决：（i）消除 ECC 带来的数据移动开销；（ii）实现平面内嵌入向量的无 ECC 无错距离计算，REIS 在 ANNS 引擎中采用**混合 SSD 架构**技术。具体而言，通过软分区方式构建两类存储分区：一是高可靠性的无 ECC 单层单元闪存（SLC）分区，用于存储二进制嵌入向量；二是常规的高密度 TLC 分区，用于存储数据库的文档块，以及无需在平面内处理的嵌入向量（如重排序所用的 8 位整型嵌入向量）。为进一步提升 SLC 分区的可靠性，REIS 采用**增强型 SLC 模式编程技术（ESP）**，实现无 ECC 下的零比特错误率（BER）。此外，相较于 TLC，SLC 的读取延迟更低 ，这一特性也能小幅提升 RAG 的整体性能。
### 嵌入-文档关联映射机制
数据库布局通过分离高频访问的嵌入向量与低频访问的文档块提升性能，但文档检索需建立二者的关联映射。为此，REIS 在存储系统内设计了一种低成本关联机制，将每个嵌入向量与其对应文档块的物理地址绑定。现代 NAND 闪存会为每一页预留专属存储空间用于存放 ECC 校验位，该区域称为**带外区（OOB）**（例如，每 16KB 物理页配备 2208 字节空闲空间 ），闪存页缓冲在读取数据页时，会同步加载带外区数据。REIS 复用带外区的少量空间，存储每个嵌入向量对应的文档块物理地址（图 4 中③）。该机制确保嵌入向量加载至页缓冲时，其关联文档块的地址也同步完成加载，因此当存储系统对整页嵌入向量执行距离计算时，页缓冲中已留存关联文档块的地址，可直接用于文档的定位与检索。该关联机制无需为文档检索维护专用数据结构，仅为存储系统带来极低的空间开销。
### 粗粒度访问机制
为实现两大目标：（i）在存储系统中区分不同的 RAG 数据集；（ii）减少嵌入向量访问过程中频繁的地址转换（L2P）开销，REIS 设计了**粗粒度访问机制**。具体实现为：REIS 在 SSD 内部 DRAM 中，为数据库的每个分区存储一条地址信息项，每条信息项包含一个整型索引TAG（作为数据集的唯一标识），以及嵌入向量区、文档区的首 / 尾地址。该粗粒度访问机制从两方面实现高效的数据库管理：第一，数据库部署阶段，存储系统先根据数据集规模，预留两个互不重叠的连续存储区域，并生成对应的地址信息项，再将数据集写入存储，确保该数据集与其他用户数据、数据集相互隔离；第二，数据库检索阶段，存储系统通过地址信息项获取数据集的嵌入向量起始地址，启动检索流程，后续读取每一页数据时，SSD 控制器只需对当前地址执行自增操作，即可推导下一页的读取地址，无需频繁调用逻辑地址 - 物理地址（L2P）映射表执行地址转换。

为保障数据完整性，REIS 保留了页级 FTL 元数据，该元数据包含数据刷新、损耗均衡等操作所需的核心信息，仅在**数据库初始化写入阶段**与**低频的周期性维护操作**（如数据刷新，通常每年一次 ）中调用，操作完成后，FTL 元数据即从 SSD 的 DRAM 中刷除。粗粒度访问机制使得数据库部署完成后，无需为两个分区持续维护页级 FTL，从而节省 SSD 内部宝贵的 DRAM 空间，用于支撑其他运算（详见后文）。举例而言，对于一个 1TB 的向量数据库，传统方案需 1GB DRAM 存储页级 FTL ，而 REIS 的地址管理开销仅降至 21 字节。鉴于 REIS 需支持多类不同 RAG 数据集的检索服务，我们将数据集的关键信息（即数据集整型索引、嵌入向量区与文档区的首 / 尾地址项）存储于 SSD 控制器 DRAM 中的一个小型数组，该数据结构称为**R-DB（检索数据库，图 4 中④）**，作为已部署数据集的管理记录表。粗粒度访问机制存在一个潜在短板：需占用连续的大容量存储区块，因此数据库部署阶段可能需要执行碎片整理操作，但该开销为一次性前置成本，可在后续长期使用中分摊抵消。
## ISP友好的ANNS算法
由于存储内处理（ISP）硬件的算力受限（如不支持浮点运算 ），基于 ISP 的 ANNS 必须采用量化技术。我们分析 IVF 与 HNSW 结合二进制量化（BQ）、乘积量化（PQ）并搭配重排序后的性能表现，得出四项核心结论：（i）IVF 即便采用二进制量化（乘积量化），召回率仍能保持 0.97（0.96）的高位；（ii）乘积量化的性能表现不及二进制量化，甚至逊色于浮点精度的 IVF；（iii）IVF 的吞吐量在二进制量化加持下实现大幅提升；（iv）HNSW 的吞吐量在二进制量化后基本保持不变，且性能仍比 IVF 高出约 3 倍。尽管上述结果表明 HNSW 与 IVF 均为适配 ANNS-based RAG 的优质方案，但 HNSW 这类图基算法存在不规则数据访问模式，会导致 SSD 内部带宽无法充分利用，因此不适用于 ISP 架构；反观 IVF，其检索过程基于连续数据执行，呈现流式访问特征，更适配 ISP 的硬件特性。据此，我们选定 IVF 作为核心检索算法，并对数据库布局进行针对性改造，以支撑该算法的高效执行。

为加速检索流程，REIS 基于倒排文件（IVF）算法实现 ANNS 检索。REIS 采用的 IVF 算法结合了量化与重排序技术，需同时存储二进制与 8 位整型（INT8）精度的嵌入向量。为高效支撑该优化版 IVF 算法的运行，我们对之前的基础数据库布局做三方面改造：

第一，将嵌入向量区拆分为三个子区域，分别用于存储聚类质心、二进制精度嵌入向量与 INT8 精度嵌入向量

第二，为便于 IVF 检索操作的执行，构建一个聚类管理数组，用于记录所有聚类的核心信息。数组中每个元素对应一个 IVF 聚类，包含三项内容：（i）该聚类质心的物理地址；（ii）聚类内首个与末个嵌入向量的索引；（iii）聚类对应的 8 比特标签TAG。我们将该数组命名为**R-IVF（检索型 IVF，图 4 中⑤）**，并存储于 SSD 的内部 DRAM 中，其内存开销为「聚类数量 ×15 字节」；

第三，对之前的嵌入 - 文档关联映射机制做两项针对性扩展：（Ⅰ）为实现二进制嵌入向量与对应 INT8 嵌入向量的关联以支撑重排序，除在带外（OOB）区存储每个嵌入向量对应的文档地址外，额外增加 INT8 嵌入向量的物理地址（RADR）存储；（Ⅱ）为满足后续运算需求，将聚类的 8 比特标签存储至该聚类质心所在物理页的带外区。IVF 算法的落地还需在 SSD 控制器的 DRAM 中分配专用数据结构：具体而言，REIS 在执行 IVF 运算期间，会维护两类列表，分别存储（i）候选聚类信息，（ii）嵌入向量及其与查询向量的距离值。这类数据结构被称为**临时最优列表（TTL，图 4 中⑥）**，将在后续检索过程中发挥核心作用。
## 存内ANNS引擎
![image.png](https://raw.githubusercontent.com/waibibab-cs/blog_img/main/cdnimg/20260101164809.png)
现有基于存储内处理（ISP）的 ANNS 加速器通常集成乘累加（MAC）单元，用于计算 ANNS 所需的欧氏距离。这类存储系统改造会带来两大问题：（i）显著的功耗与芯片面积开销；（ii）硬件改动具有侵入性，阻碍实际落地。REIS 的设计目标是规避现有方案的功耗与面积开销，为此我们基于二进制量化设计了一款存储内 ANNS 引擎，其核心特性为：（i）仅利用 SSD 系统的现有组件完成检索；（ii）充分利用存储系统的平面级、芯片级与通道级并行能力；（iii）集成距离过滤与流水线两项关键优化。
### 检索流程
倒排文件（IVF）算法的检索流程分为粗粒度搜索与细粒度搜索两步：

第一步，粗粒度搜索阶段，REIS 遍历所有聚类质心，筛选出与查询嵌入向量最相似的质心。具体流程为：REIS 先读取某一页的所有质心嵌入向量，计算其与查询向量的距离；对每个质心，生成包含「距离值（DIST）、嵌入向量（EMB）、嵌入向量地址（EADR）、聚类标签（TAG）」的条目，并将这些条目存入 SSD 内部 DRAM 中的**质心临时最优列表（TTL-C）**。每读取一页并填充 TTL-C 后，SSD 控制器的嵌入式核心会执行快速选择（quickselect）内核，基于距离值筛选出与查询最接近的 N 个聚类对应的条目。在此期间，存储系统会并行读取下一页质心并执行距离计算，以隐藏选择操作的延迟。每一轮迭代包含（i）页读取、（ii）距离计算、（iii）嵌入向量筛选三个步骤，并通过新的近邻聚类更新 TTL-C。最后一轮迭代完成后，REIS 基于最终的 TTL-C 确定候选聚类。

第二步，细粒度搜索阶段，REIS 在第一步筛选出的候选聚类内执行检索，与粗粒度搜索相比有两点核心差异：（i）细粒度搜索的临时最优列表条目不再包含 TAG，而是由「距离值（DIST）、嵌入向量（EMB）、INT8 嵌入向量地址（RADR）、关联文档地址（DADR）」组成，该列表被命名为**嵌入向量临时最优列表（TTL-E）**；（ii）在筛选出与查询最接近的 k 个嵌入向量后，存储系统执行快速排序（quicksort），得到按距离排序的前 k 个结果列表。
### 检索架构与执行流程
图 6 将 REIS 的执行流程拆解为 9 个步骤：

首先，存储系统将 DRAM 中的查询嵌入向量传输至每个 NAND 闪存平面的数据缓冲器①，并写入多份副本以填满整个缓存锁存器（CL）。这些副本与数据库嵌入向量对齐，以便执行后续的位运算，该步骤被称为**输入广播（IBC）**。输入广播完成后，每个 CL 中会存有 N 份查询副本（N = 页大小 / 嵌入向量大小）。

步骤 2 中，存储系统向每个plane发送页读取命令，将一页数据库嵌入向量加载至传感锁存器（SL）。步骤 3 中，通过对 CL（存储查询嵌入向量）与 SL（存储数据库嵌入向量）执行异或（XOR）运算，并将结果存入数据锁存器（DL），每个平面完成查询向量与数据库嵌入向量的按位差计算。

步骤 4 中，利用闪存外围逻辑中的**失效位计数器**，统计 DL 中逻辑 “1” 的数量，该数值对应查询向量与数据库嵌入向量的距离。从闪存芯片传输至 SSD 控制器 DRAM 的数据内容，会根据步骤 1-4 是执行粗粒度还是细粒度搜索而变化：粗粒度搜索时，ANNS 引擎传输的条目包含（i）嵌入向量（EMB）和嵌入向量地址（EADR）、（ii）计算出的距离（DIST）、（iii）该嵌入向量所属聚类的标签（TAG）；细粒度搜索时，不再传输聚类标签，而是额外传输（iv）INT8 精度嵌入向量的地址（RADR）、（v）关联文档块的地址（DADR）。

步骤 2-4 会重复执行，直至遍历完整个数据库。SSD 控制器从临时最优列表（TTL）中读取距离值，利用嵌入式核心执行快速选择算法⑥，筛选出与查询最接近的 10k 个嵌入向量。

步骤 7 中，SSD 控制器的嵌入式核心执行**重排序内核**。重排序会对 ANNS 筛选出的候选数据执行计算成本更高但精度更优的检索：重排序器通常采用（i）交叉编码器模型精准计算查询与文档块的相似度，或（ii）以更高精度（如 INT8）重新计算距离。REIS 采用第二种方案：ANNS 基于二进制量化执行，重排序则基于 INT8 精度嵌入向量完成。具体流程为：嵌入式核心通过 RADR 从 INT8 嵌入向量区读取前 10k 个嵌入向量，以 INT8 精度重新计算距离，并通过快速排序⑧得到排序后的前 k 个嵌入向量，检索流程至此结束。

ANNS 检索完成后，ANNS 引擎根据前 k 个结果的 DADR 定位相关文档块，并将其传输至主机系统⑨，供后续生成阶段使用。

本人对整个过程的理解：**先根据r-db读取每个质心页，粗粒度搜索得到候选聚类，再根据r-ivf得到这些候选聚类的所有嵌入，再进行细粒度搜索，重排序后拿到对应的文档返回**

一些优化细节：
**利用 SSD 并行能力**：REIS 借助存储系统平面与芯片内的缓冲器和外围逻辑执行距离计算，支持多平面、多芯片同时执行异或与位计数运算，充分利用存储系统的并行能力。计算完成后，存储系统的闪存通道可通过通道级并行，提供海量内部带宽，高效地将条目从闪存芯片传输至 SSD 控制器的 DRAM。

**细粒度嵌入向量访问**：为实现嵌入向量的细粒度访问，REIS 引入**迷你页（Mini-Page）** 寻址机制：在原始物理页地址后追加偏移量，组成迷你页地址，使每一页能存储尽可能多的嵌入向量（例如，16KB 页可存储 128 个 1024 维二进制嵌入向量，对应 7 位迷你页地址偏移量）。ANNS 引擎执行检索时，会将迷你页地址作为临时最优列表（TTL）中每个条目的嵌入向量地址（EADR）。
### 距离过滤
实验发现，对于任意查询，数据库中大部分文档块均为无关内容（即其嵌入向量与查询向量的距离极大）。为避免将无关数据传输至 SSD 控制器，REIS 引入**距离过滤**机制：当嵌入向量与查询向量的距离超过预设阈值时，直接丢弃该嵌入向量。该机制可（i）节省 SSD 通道带宽；（ii）减少 SSD 控制器需筛选与排序的条目数量。

我们对上述步骤 4 进行改造，将距离过滤集成至 ANNS 内核：为确定合适的阈值，在 4 个面向不同检索任务的 BEIR数据集（HotpotQA 、NQ 、FEVER 、Quora ）上开展过滤实验，得出两项结论：第一，在 HotpotQA 数据集中，过滤掉 99% 的文档后，仍能检索到每个查询对应的 k=10 个最相关文档；第二，过滤阈值的选择与数据集规模弱相关 —— 对于 k=10 的场景，最大数据集 FEVER 的阈值仅比最小数据集 Quora 高 1.6%。由此可知：（i）距离过滤可显著减少候选嵌入向量数量，降低计算开销；（ii）可采用统一阈值对不同规模的数据集进行有效过滤。

REIS 利用闪存芯片内的比较器逻辑（即通过 / 失效检查器）实现距离过滤：将距离值与预设阈值比较，仅将距离值低于阈值的嵌入向量传输至 SSD 的 DRAM，用于后续处理。
### 流水线优化
为进一步加速 RAG 检索，REIS 利用存储系统的三项流水线优化机会：

第一，利用闪存芯片中广泛支持的**页缓存顺序读取模式**，将步骤 2-4 两轮迭代的操作重叠执行。具体而言，在步骤 4 中，当外围逻辑（PL）将数据传输至 DL 以供读取后，可立即读取下一页数据；

第二，将 NAND 闪存芯片上的距离计算与嵌入式核心上的内核执行操作重叠。评估表明，单个核心即可高效运行快速排序与重排序，不会导致流水线阻塞。因此，REIS 仅用一个核心执行快速排序与重排序，其余核心（例如 4 核中的 3 个 ）仍可处理常规 SSD 操作；

第三，在输入广播（IBC）阶段，REIS 支持单芯片内的所有平面同时接收输入查询，该优化被命名为**多平面输入广播（MPIBC）**。此优化可将输入广播延迟降低至原延迟的 “1 / 每芯片平面数”。我们假设芯片外围的专用多路复用器逻辑负责平面选择，因此启用 MPIBC 仅需同时激活所有平面的选择信号，使各平面可并发接收输入查询嵌入向量。
## REIS的系统集成
为实现与主机的通信，REIS 设计了一套应用程序编程接口（API），在 NVM 命令集的基础上扩展了 RAG 专属操作指令。同理，为支撑SSD内的运算流程，REIS 对 NAND 闪存命令集进行了扩展，新增了控制器与闪存芯片间的通信指令。
### 应用程序编程接口
![image.png](https://raw.githubusercontent.com/waibibab-cs/blog_img/main/cdnimg/20260101175137.png)
REIS 为主机系统定义了一套高层级 API，用于执行 RAG 工作流的索引阶段与检索阶段。为此，我们在 NVM 命令集中新增了 REIS 专属操作指令 —— 该命令集规范为厂商自定义指令预留了操作码范围（80h~FFh），足以支撑所有 REIS 操作的实现。

在**索引阶段**，主机系统向 SSD 发送`DB_Deploy()`（或`IVF_Deploy()`）指令：REIS 会根据 API 要求在 NAND 闪存中预留所需空间，并执行碎片整理操作以确保物理地址的连续性；随后等待主机将数据库内容写入 SSD 的 DRAM，再按照之前所述流程将数据写入存储介质。

在**检索阶段**，当 REIS 接收到主机发送的`Search()`（或`IVF_Search()`）指令后，启动检索流程；待定位到需返回的文档块后，向主机发送 “完成信号”；主机确认该信号后，存储系统开始将已定位的文档块传输至主机。表 1 详细描述了各 API 指令的功能。
### NAND Flash命令集
![image.png](https://raw.githubusercontent.com/waibibab-cs/blog_img/main/cdnimg/20260101175256.png)
REIS 在 NAND 闪存芯片的控制逻辑中新增了指令，以支撑存储内 ANNS 引擎的检索操作。具体流程为：控制器先接收前述 API 指令，并将其转换为闪存命令；随后向闪存芯片发送这些命令，触发相应操作。每个闪存芯片内的控制逻辑是一个有限状态机，负责接收命令并控制闪存阵列的外围逻辑执行操作。表 2 详细描述了为数据库检索新增的 NAND 闪存命令集扩展。
# 实验评估
## 评估方案
### 实验配置
![image.png](https://raw.githubusercontent.com/waibibab-cs/blog_img/main/cdnimg/20260101175747.png)
我们基于两款商用 SSD 产品（Samsung PM9A3 与 Micron 9400 ），在 REIS-SSD1 与 REIS-SSD2 两种 SSD 配置下评估 REIS—— 这两款 SSD 分别侧重低成本与高性能。文档检索的基准系统采用高端服务器，配备 AMD EPYC 9554 CPU与 Samsung PM9A3 SSD 。表 3 列出了 SSD 与基准 CPU 系统（CPU-Real）的参数。

为凸显数据库布局与存储内处理（In-Storage Processing）带来的性能提升，我们首先基于暴力搜索（BF）对比 REIS 与 CPU-Real；随后在近似最近邻搜索场景下对比两者。我们使用 FAISS 库提供的 BQ 量化与重排序方案，在 IVF 算法下评估 REIS 与 CPU-Real，并将 IVF 的 Recall@10 精度从 0.98 调整至 0.9。

为开展敏感性分析，我们引入 No-OPT 作为基准配置：该 REIS 配置使用存储内 ANNS 引擎，但不启用距离过滤（DF）、流水线（PL）与多平面输入广播（MPIBC）。为单独量化 ANNS 带来的性能开销，我们基于 CPU 基准新增一个对比项 No-I/O：该配置消除了存储 I/O 数据移动带来的开销。此外，我们将 REIS 与两款最先进设计（NDSearch 与 ICE ）对比 —— 两者分别采用图基与聚类基 ANNS 算法。必要时我们会调整实验方法以确保公平性。
### 性能与能耗评估平台
我们的 SSD 操作模型与参数基于 Flash-Cosmos ，SSD 内部 DRAM 的建模使用 CACTI7 ；嵌入式 SSD 控制器核心的仿真采用 Zsim 与 Ramulator 。SSD 功耗基于商用产品 与 Flash-Cosmos 的真实芯片特性数据建模；SSD 内部 DRAM 与嵌入式核心的功耗，分别通过 CACTI7与商用嵌入式 SSD 控制器处理器的特性推导得出。CPU-Real 的功耗测量中，CPU 部分使用 AMD μProf，DRAM 部分采用 DDR4 模型。
### 数据集
我们使用信息检索基准中的两个数据集（NQ 与 HotpotQA）、基于维基百科的公开数据集（wiki_full）及其英文子集（wiki_en）。在与 NDSearch对比时，我们采用其评估中使用的两个十亿级数据集（SIFT1B 与 DEEP1B）。
## 实验结果
### 性能表现
![image.png](https://raw.githubusercontent.com/waibibab-cs/blog_img/main/cdnimg/20260101233603.png)

图 7 展示 REIS 的检索性能，以每秒查询数（QPS）为单位，且结果均归一化至 CPU-Real 的性能水平。我们得出三项关键结论：其一，REIS-SSD1 和 REIS-SSD2 相较 CPU-Real，性能平均提升**13 倍**，最高达**112 倍**，印证了 REIS 缓解 RAG 检索流程 I/O 瓶颈的显著成效；其二，得益于 REIS 充分利用存储系统的海量内部并行能力，REIS-SSD1 和 REIS-SSD2 相较 No-I/O，性能平均提升**1.8 倍**，最高达**5.3 倍**；其三，REIS-SSD2 相较 REIS-SSD1，性能平均提速**2.6 倍**，最高达**3.2 倍**，这一提升体现了更高通道数（2 倍）与通道带宽（1.7 倍）带来的性能增益。
### 能效表现
![image.png](https://raw.githubusercontent.com/waibibab-cs/blog_img/main/cdnimg/20260101233656.png)
图 8 展示 REIS 的能效表现（QPS/W，每瓦每秒查询数），结果均归一化至 CPU-Real 的能效水平。我们得出两项关键结论：其一，REIS-SSD1 和 REIS-SSD2 相较 CPU-Real，能效平均提升**55 倍**，最高达**157 倍**；该能效提升的核心原因是，SSD 的功耗相较 CPU 基准系统平均降低**29.7 倍**。其二，REIS-SSD2 相较 REIS-SSD1，能效平均提升**2.2 倍**，最高达**2.6 倍**；这一能效提升幅度与 REIS-SSD2 相对 REIS-SSD1 的性能提升幅度相近，表明 REIS-SSD2 的能效增益主要源于其设计带来的更高吞吐量。
### 端到端性能分析
![image.png](https://raw.githubusercontent.com/waibibab-cs/blog_img/main/cdnimg/20260101233915.png)
表 4 拆解了 REIS-SSD1 与基于 CPU 的二进制量化系统上，RAG 流水线各阶段的时延分布。其中数据集采用 HotpotQA 与 wiki_en。
由于 REIS 在存储系统内部完成检索流程，无需执行 Dataset Loading 步骤，该步骤需将数据传输至主机的 DRAM。实验可见，REIS 将 Dataset Loading 与 Search 阶段的合并时延占比，从 20.3%~69.3% 降至 0.02%~0.15%，这一结果印证了 REIS 可高效消除 RAG 检索中的 data movement bottleneck。
在启用 REIS 的情况下，Generation 占总耗时的 92%，这表明 LLM inference 已成为新的性能瓶颈。整体而言，REIS 使 HotpotQA 与 NQ 数据集的平均 end-to-end latency 分别降低 1.25× 和 3.24×。
### 敏感性研究
![image.png](https://raw.githubusercontent.com/waibibab-cs/blog_img/main/cdnimg/20260101234022.png)
图 9 展示了 REIS 所提出的所有优化策略的敏感性分析结果，即在 No-OPT 基础上叠加距离过滤（DF）、流水线（PL）和多平面输入广播（MPIBC）后的性能表现。

实验选取 wiki_full作为数据集，所有结果（即 QPS）均归一化至 CPU-Real 的性能水平。我们得出三项结论：第一，在所有提出的优化策略中，DF 为 No-OPT 带来的性能提升最为显著，REIS-SSD1 的性能平均提升 4.7 倍、最高提升 5.1 倍，REIS-SSD2 的性能平均提升 5.7 倍、最高提升 6.5 倍。该性能提升的核心原因在于，在各 NAND 闪存芯片内过滤掉距离过大的嵌入向量，可大幅减少不必要的向 SSD 控制器 DRAM 的数据传输，同时降低输入至 Quickselect 内核的数据量。第二，PL 带来的性能增益随 SSD 内部带宽的提升而增大，这得益于更多的通道数与更高的 I/O 速率。具体而言，在内部带宽较高的 SSD 中（如 REIS-SSD2 的 32GB/s 带宽），流水线技术可实现两项操作的完全重叠：读取新的物理页，以及将过滤后的 TTL 条目从 NAND 闪存芯片传输至 SSD 内部 DRAM。第三，MPIBC 带来的性能增益随 SSD 单芯片平面数的增加而增大。具体来看，DF+PL+MPIBC 相较 DF+PL 的平均性能提升，在 REIS-SSD1 上为 6%，在 REIS-SSD2 上为 26%。
### 与先前的工作进行对比
我们将 REIS 的性能与两款当前最优的基于 ISP 的 ANNS 加速器展开对比，分别是采用聚类基算法的 ICE和采用图基算法的 NDSearch。
![image.png](https://raw.githubusercontent.com/waibibab-cs/blog_img/main/cdnimg/20260101234306.png)
图 10 展示了 REIS 相对 ICE的性能提升倍数，ICE 是面向向量相似度检索的当前最优 ISP 方案。在暴力搜索（BF）模式下，REIS 在所有配置下的性能提升均超过 10 倍；在 IVF 模式下，性能提升倍数随召回率的升高而增大，体现出相较于 ICE 的性能优势。具体而言，在 SSD-2 配置下的所有数据集上，当 Recall@10 为 0.90 时，REIS 性能平均优于 ICE7.1 倍，Recall@10 为 0.98 时，这一数值达 22.9 倍。

我们还与 ICE-ESP 进行了对比，该方案是 ICE 的理想实现版本，无需执行 ECC 校验，但仍采用 4 比特量化（相关结果未展示于图 10）。即便与 ICE-ESP 相比，REIS 在 BF 模式下，于 SSD-1（SSD-2）上实现的几何平均性能提升仍达 3.85 倍（3.92 倍）；在 IVF 模式下配置为 Recall@10=0.9 时，REIS 性能较 ICE-ESP 提升 2.08 倍（2.29 倍），当 Recall@10=0.98 时，SSD-1（SSD-2）上的性能提升进一步增至 2.84 倍（3.18 倍）。

![image.png](https://raw.githubusercontent.com/waibibab-cs/blog_img/main/cdnimg/20260101234331.png)
图 11 对比了采用 IVF的 REIS，与采用 HNSW和 DiskANN的 NDSearch 的性能表现。本次对比采用两个十亿级数据集 SIFT-1B 与 DEEP-1B，对应的 Recall@10 分别为 0.94 和 0.93。我们将 REIS 的吞吐量归一化至采用 HNSW 和 DiskANN 的 NDSearch 吞吐量水平，结果显示 REIS 性能平均优于 NDSearch1.7 倍，最高达 2.6 倍。
# 优化方向
本节探讨 REIS 的潜在扩展方向与优化方案：第一，为 REIS 增设基于用户自定义元数据的过滤检索功能；第二，分析 REIS 对 SSD 常规管理操作及使用寿命的影响；第三，提出 REIS 嵌入 - 文档关联机制的替代实现方案，缓解对物理地址连续性的要求。
## 元数据过滤
为提升生成质量，主流大语言模型部署框架均在 RAG 检索中融入元数据过滤机制 。元数据过滤为数据库条目补充时间戳、作者信息及其他相关元数据，可在检索过程中辅助优化文档召回效果。REIS 可通过在 NAND 闪存预留区域（即带外区 OOB）存储各嵌入向量的元数据，实现该功能扩展。针对只读数据库的元数据过滤场景，该增强版 REIS 的实现方式为：（1）为每个嵌入向量分配对应的元数据标签（整数型）；（2）在数据库部署阶段，将该标签写入带外区。执行 RAG 检索时，REIS 接收查询嵌入向量及目标元数据标签，利用现有嵌入向量距离计算逻辑，将查询标签与各数据库嵌入向量的标签进行比对；在执行后续检索步骤前，REIS 校验元数据比对结果，过滤掉不匹配的检索结果。

针对支持实时知识检索的动态更新数据库，REIS 的实现策略为：（1）按预设频率（如每小时）创建新数据库存储新增信息；（2）将每个子数据库视作独立数据库，为其分配唯一时间戳标签；（3）在 SSD 内部 DRAM 中为每个数据库维护条目，记录数据库地址及对应时间戳。当主机发送携带目标时间的查询请求时，REIS 首先将目标时间与内部 DRAM 存储的时间戳进行比对，确定待检索的目标数据库，随后在这些数据库内执行检索操作。
## REIS对SSD常规操作以及寿命的影响
REIS 虽以加速 RAG 为核心设计目标，同时仍可作为常规存储系统使用，因此 SSD 控制器需处理数据刷新、垃圾回收等日常维护任务。为保障维护操作不间断执行，我们采取两项设计：（1）将 REIS 的运行限制在 SSD 单个嵌入式核心中；（2）当所有核心均需参与维护时，将维护任务的优先级设为高于 RAG 操作。由于 REIS 主要面向读密集型 RAG 负载，写操作频次较低，所有核心同时被占用的情况极少出现。为简化设计，REIS 在任一时刻仅工作于 RAG 模式或 SSD 常规模式。模式切换时，需加载对应的闪存转换层数据（RAG 模式加载粗粒度数据；常规模式加载细粒度数据）。因 REIS 同一时间仅运行于一种模式，主机发起的常规读写操作性能不受任何影响。

尽管 REIS 为支持芯片内逻辑运算，在 SLC 分区中关闭了差错校验（ECC）功能，但这并不会降低 SSD 的使用寿命，原因有二：其一，采用 SLC 模式而非 MLC 模式，本质上会增大闪存单元的阈值电压间距，提升存储单元的可靠性；其二，REIS 为 SLC 分区采用嵌入式差错防护（ESP）技术，在最坏工况下（数据保存期 1 年、编程 / 擦除循环 1 万次），可实现位错误率（BER）为 0 的效果。
## 降低地址连续性的要求
粗粒度访问要求闪存存在连续的未分配物理空间。为进一步降低闪存转换层（L2P）元数据带来的内存开销与地址转换开销，REIS 在数据库的文档存储区也采用了相同的基于地址连续性的设计方案。

针对文档存储区，可采用无需地址连续性的替代实现方案：通过带外区将嵌入向量与对应文档块的物理地址建立关联，使文档块可存储于闪存任意位置。但该方案会增加设计复杂度，原因是当文档块在 SSD 内被重新映射至其他区域时（如数据更新阶段），需同步更新带外区中存储的物理地址信息。
# ANNS优化的相关工作
由于 ANNS 已被广泛应用于十亿级推荐系统，近年研究提出了专用库与优化算法 以提升其性能。这些工作通过针对处理器中心系统的各类优化，改善了 ANNS 的性能，但因面向的是处理器中心架构，无法突破 REIS 所针对的 I/O 数据移动瓶颈。

各类 ANNS 硬件加速器采用了内存扩展、多节点并行等方案。存内处理（PIM）技术也被用于加速近邻搜索：例如，IKS提出一种基于 CXL 的设备，将向量积加速器部署在 LPDDR 内存附近，以提升精确近邻搜索（ENNS）的性能；在文献（Robust implementation of retrieval-augmented generation on edge-based computing-in-memory architectures）中，Qin 等人利用非易失性内存技术的特性，在模拟域执行矩阵 - 向量乘法，以加速边缘设备中的 RAG 流水线。

尽管上述方案提升了性能，但基于 DRAM 的方法要么无法从根本上解决存储带来的 I/O 数据移动瓶颈，要么在处理大规模数据集时会产生高昂成本。